{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title       : \"Empirical likelihood\"\n",
    "subtitle    :\n",
    "author      : Paul Schrimpf\n",
    "date        : `j using Dates; print(Dates.today())`\n",
    "bibliography: \"el.bib\"\n",
    "---\n",
    "\n",
    "[![](https://i.creativecommons.org/l/by-sa/4.0/88x31.png)](http://creativecommons.org/licenses/by-sa/4.0/)\n",
    "\n",
    "This work is licensed under a [Creative Commons Attribution-ShareAlike\n",
    "4.0 International\n",
    "License](http://creativecommons.org/licenses/by-sa/4.0/) \n",
    "\n",
    "### About this document {-}\n",
    "\n",
    "This document was created using Weave.jl. The code is available in\n",
    "[on github](https://github.com/schrimpf/GMMInference.jl). The same\n",
    "document generates both static webpages and associated [jupyter\n",
    "notebook](empiricalLikelihood.ipynb).\n",
    "\n",
    "$$\n",
    "\\def\\indep{\\perp\\!\\!\\!\\perp}\n",
    "\\def\\Er{\\mathrm{E}}\n",
    "\\def\\R{\\mathbb{R}}\n",
    "\\def\\En{{\\mathbb{E}_n}}\n",
    "\\def\\Pr{\\mathrm{P}}\n",
    "\\newcommand{\\norm}[1]{\\left\\Vert {#1} \\right\\Vert}\n",
    "\\newcommand{\\abs}[1]{\\left\\vert {#1} \\right\\vert}\n",
    "\\DeclareMathOperator*{\\argmax}{arg\\,max}\n",
    "\\DeclareMathOperator*{\\argmin}{arg\\,min}\n",
    "\\def\\inprob{\\,{\\buildrel p \\over \\rightarrow}\\,} \n",
    "\\def\\indist{\\,{\\buildrel d \\over \\rightarrow}\\,} \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Agg\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "markdown = try\n",
    "  \"md\" in keys(WEAVE_ARGS) && WEAVE_ARGS[\"md\"]\n",
    "catch\n",
    "  false\n",
    "end\n",
    "\n",
    "if !(\"DISPLAY\" ∈ keys(ENV))\n",
    "  ENV[\"GKSwstype\"]=\"nul\"\n",
    "  ENV[\"MPLBACKEND\"]=\"Agg\"\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Empirical likelihood\n",
    "\n",
    "An interesting alternative to GMM is (generalized) empirical\n",
    "likelihood (GEL). Empirical likelihood has some appealing higher-order\n",
    "statistical properties. In particular, it can be shown to have lower\n",
    "order asymptotic bias than GMM. See @newey2004. Relatedly, certain\n",
    "test statistics based on EL are robust to weak identification\n",
    "[@guggenberger2005]. In fact, the identification robust tests that we\n",
    "have discusses are all based on the CUE-GMM objective function. The\n",
    "CUE-GMM objetive is a special case of generalized empirical\n",
    "likelihood. \n",
    "\n",
    "A perceived downside of GEL is that it involves a more difficult\n",
    "looking optimization problem than GMM. However, given the ease with\n",
    "which Julia can solve high dimensional optimization problems, GEL is\n",
    "very feasible. \n",
    "\n",
    "As in the extremum estimation notes, suppose we have moment conditions\n",
    "such that\n",
    "$$\n",
    "\\Er[g_i(\\theta)] = 0\n",
    "$$\n",
    "where $g_i:\\R^p \\to \\R^k$ are some data dependent moment\n",
    "conditions. The empirical likelihood estimator solves\n",
    "$$\n",
    "\\begin{align*}\n",
    "    (\\hat{\\theta}, \\hat{\\pi}) = & \\argmax_{\\theta,\\pi} \\frac{1}{n} \\sum_i\n",
    "    \\log(\\pi_i) \\;\\; s.t.  \\\\\n",
    "     & \\frac{1}{n} \\sum_i \\pi_i = 1, \\;\\; 0\\leq \\pi \\leq 1 \\\\\n",
    "     & \\sum_i \\pi_i g_i(\\theta) = 0 \n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Generalized empirical likelihood replaces $\\log(\\pi)$ with some other\n",
    "convex function $h(\\pi)$, \n",
    "$$\n",
    "\\begin{align*}\n",
    "    (\\hat{\\theta}^{GEL,h}, \\hat{\\pi}) = & \\argmin_{\\theta,\\pi}\n",
    "                                          \\frac{1}{n} \\sum_i h(\\pi_i) \\;\\; s.t.  \\\\\n",
    "     & \\frac{1}{n} \\sum_i \\pi_i = 1, \\;\\; 0\\leq \\pi \\leq 1 \\\\\n",
    "     & \\sum_i \\pi_i g_i(\\theta) = 0 \n",
    "\\end{align*}\n",
    "$$\n",
    "setting $h(\\pi) = \\frac{1}{2}(\\pi^2-(1/n)^2)$ results in an estimator\n",
    "identical to the CUE-GMM estimator.\n",
    "\n",
    "A common approach to computing GEL estimators is to eliminate $\\pi$ by\n",
    "looking at the dual problem\n",
    "$$\n",
    "\\hat{\\theta}^{GEL}  = \\argmin_{\\theta}\\sup_\\lambda \\sum_i \\rho(\\lambda'g_i(\\theta))\n",
    "$$\n",
    "where $\\rho$ is some function related to $h$. See Newey and Smith (2004)[@newey2004] for\n",
    "details. There can be some analytic advantages to doing so, but\n",
    "computationally, the original statement of the problem has some\n",
    "advantages. First, there is more existing software for solving\n",
    "constrained minimization problems than for solving saddle point\n",
    "problems. Second, although $\\pi$ is high dimensional, it enters the\n",
    "constraints linearly, and the objective function is concave. Many\n",
    "optimization algorithms will take good advantage of this. \n",
    "\n",
    "Let's look at some Julia code. Since the problem involves many\n",
    "variables with linear constraints, it is worthwhile to use JuMP for\n",
    "optimization. The code is very slightly more verbose, but the speed of\n",
    "JuMP (and the Ipopt solver) are worth it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m = A JuMP Model\n",
      "Maximization problem with:\n",
      "Variables: 1004\n",
      "Objective function type: Nonlinear\n",
      "`GenericAffExpr{Float64,VariableRef}`-in-`MathOptInterface.EqualTo{Float64}`: 1 constraint\n",
      "`GenericQuadExpr{Float64,VariableRef}`-in-`MathOptInterface.EqualTo{Float64}`: 8 constraints\n",
      "`VariableRef`-in-`MathOptInterface.GreaterThan{Float64}`: 1000 constraints\n",
      "`VariableRef`-in-`MathOptInterface.LessThan{Float64}`: 1000 constraints\n",
      "Model mode: AUTOMATIC\n",
      "CachingOptimizer state: EMPTY_OPTIMIZER\n",
      "Solver name: Ipopt\n",
      "Names registered in the model: momentcon, p, prob, θ\n",
      "\n",
      "******************************************************************************\n",
      "This program contains Ipopt, a library for large-scale nonlinear optimization.\n",
      " Ipopt is released as open source code under the Eclipse Public License (EPL).\n",
      "         For more information visit http://projects.coin-or.org/Ipopt\n",
      "******************************************************************************\n",
      "\n",
      "This is Ipopt version 3.12.10, running with linear solver mumps.\n",
      "NOTE: Other linear solvers might be more efficient (see Ipopt documentation).\n",
      "\n",
      "Number of nonzeros in equality constraint Jacobian...:    73000\n",
      "Number of nonzeros in inequality constraint Jacobian.:        0\n",
      "Number of nonzeros in Lagrangian Hessian.............:    33000\n",
      "\n",
      "Total number of variables............................:     1004\n",
      "                     variables with only lower bounds:        0\n",
      "                variables with lower and upper bounds:     1000\n",
      "                     variables with only upper bounds:        0\n",
      "Total number of equality constraints.................:        9\n",
      "Total number of inequality constraints...............:        0\n",
      "        inequality constraints with only lower bounds:        0\n",
      "   inequality constraints with lower and upper bounds:        0\n",
      "        inequality constraints with only upper bounds:        0\n",
      "\n",
      "iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n",
      "   0  4.6051712e+03 4.12e+01 1.26e-13  -1.0 0.00e+00    -  0.00e+00 0.00e+00   0\n",
      "   1  6.9077800e+03 3.70e+00 8.40e+01  -1.0 1.02e-01    -  1.00e+00 1.00e+00h  1\n",
      "   2  6.9078636e+03 2.73e-02 1.53e+01  -1.0 9.19e-01    -  1.00e+00 1.00e+00h  1\n",
      "   3  6.9094032e+03 7.00e-05 1.51e+01  -1.0 4.75e-03    -  1.00e+00 1.00e+00h  1\n",
      "   4  6.9093711e+03 1.99e-06 1.79e+00  -1.0 1.40e-03    -  1.00e+00 1.00e+00h  1\n",
      "   5  6.9093710e+03 1.60e-09 5.73e-03  -1.0 3.11e-05    -  1.00e+00 1.00e+00h  1\n",
      "   6  6.9093710e+03 2.42e-14 5.87e-06  -2.5 9.49e-08    -  1.00e+00 1.00e+00h  1\n",
      "   7  6.9093710e+03 1.32e-14 1.50e-07  -3.8 8.80e-08    -  1.00e+00 1.00e+00h  1\n",
      "   8  6.9093710e+03 1.38e-14 2.22e-10  -5.7 2.35e-09    -  1.00e+00 1.00e+00h  1\n",
      "   9  6.9093710e+03 1.03e-14 1.25e-13  -8.6 3.51e-12    -  1.00e+00 1.00e+00h  1\n",
      "\n",
      "Number of Iterations....: 9\n",
      "\n",
      "                                   (scaled)                 (unscaled)\n",
      "Objective...............:   6.9093710195931726e+02    6.9093710195931726e+03\n",
      "Dual infeasibility......:   1.2526083037937917e-13    1.2526083037937917e-12\n",
      "Constraint violation....:   9.0046741911750356e-15    1.0299595377960546e-14\n",
      "Complementarity.........:   2.5059035828322995e-09    2.5059035828322993e-08\n",
      "Overall NLP error.......:   2.5059035828322995e-09    2.5059035828322993e-08\n",
      "\n",
      "\n",
      "Number of objective function evaluations             = 10\n",
      "Number of objective gradient evaluations             = 10\n",
      "Number of equality constraint evaluations            = 10\n",
      "Number of inequality constraint evaluations          = 0\n",
      "Number of equality constraint Jacobian evaluations   = 10\n",
      "Number of inequality constraint Jacobian evaluations = 0\n",
      "Number of Lagrangian Hessian evaluations             = 9\n",
      "Total CPU secs in IPOPT (w/o function evaluations)   =      2.507\n",
      "Total CPU secs in NLP function evaluations           =      0.701\n",
      "\n",
      "EXIT: Optimal Solution Found.\n",
      "value.(m[:θ]) = [0.9889491611396567, 1.0354541968981905, 0.9794672562402308, 0.9873180936823952]\n",
      "value.((m[:p])[1:10]) = [0.000981706994525849, 0.0010056151388670891, 0.0009900447186719902, 0.0009959797646481284, 0.000988396648046758, 0.0010868309642819778, 0.0010060450623632711, 0.0010037501343294634, 0.0009890652900531175, 0.0010201668798391328]\n",
      "  79.792 ms (25813 allocations: 6.11 MiB)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Nothing[nothing, nothing, nothing, nothing], Nothing[nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing  …  nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing], nothing)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using JuMP, Ipopt, LinearAlgebra, Distributions, Plots\n",
    "Plots.gr()\n",
    "\n",
    "function simulate_ivshare(n,β,γ,ρ)\n",
    "  z = randn(n, size(γ)[1])\n",
    "  endo = randn(n, length(β))\n",
    "  x = z*γ .+ endo\n",
    "  ξ = rand(Normal(0,sqrt((1.0-ρ^2))),n).+endo[:,1]*ρ\n",
    "  y = cdf.(Logistic(), x*β .+ ξ)\n",
    "  return((y=y,x=x,z=z))\n",
    "end\n",
    "n = 1000\n",
    "k = 4\n",
    "iv = 2*k\n",
    "β0 = ones(k)\n",
    "π0 = vcat(I,ones(iv-k,k))\n",
    "ρ = 0.5\n",
    "(y,x,z) = simulate_ivshare(n,β0,π0,ρ)\n",
    "\n",
    "function gi_ivshare(β,y,x,z)\n",
    "  ξ = quantile.(Logistic(),y) .- x*β\n",
    "  ξ.*z\n",
    "end\n",
    "\n",
    "function gel_jump(y,x,z)\n",
    "  n,d = size(x)\n",
    "  n,k = size(z)\n",
    "  Ty = quantile.(Logistic(),y)   \n",
    "  m = Model(with_optimizer(Ipopt.Optimizer, print_level=5))\n",
    "  @variable(m, 0.0 <= p[1:n] <= 1.0)\n",
    "  @variable(m, θ[1:d])\n",
    "  @constraint(m, prob,sum(p)==1.0)\n",
    "  @constraint(m, momentcon[i=1:k], dot((Ty - x*θ).*z[:,i],p)==0.0)\n",
    "  @NLobjective(m,Max, sum(log(p[i]) for i in 1:n))\n",
    "  return(m)\n",
    "end\n",
    "\n",
    "m = gel_jump(y,x,z)\n",
    "set_start_value.(m[:θ], 2.0)\n",
    "set_start_value.(m[:p], 1/n)\n",
    "@show m\n",
    "optimize!(m)\n",
    "@show value.(m[:θ])\n",
    "@show value.(m[:p][1:10])\n",
    "\n",
    "using BenchmarkTools\n",
    "set_optimizer(m, with_optimizer(Ipopt.Optimizer,print_level=0))\n",
    "@btime (()->(set_start_value.(m[:θ],2.0),\n",
    "             set_start_value.(m[:p],1/n),\n",
    "             optimize!(m)))()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For comparison here is how long it takes JuMP + Ipopt to solve for the\n",
    "CUE-GMM estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mcue = cue_jump(y, x, z) = A JuMP Model\n",
      "Minimization problem with:\n",
      "Variables: 4\n",
      "Objective function type: Nonlinear\n",
      "Model mode: AUTOMATIC\n",
      "CachingOptimizer state: EMPTY_OPTIMIZER\n",
      "Solver name: Ipopt\n",
      "Names registered in the model: θ\n",
      "This is Ipopt version 3.12.10, running with linear solver mumps.\n",
      "NOTE: Other linear solvers might be more efficient (see Ipopt documentation).\n",
      "\n",
      "Number of nonzeros in equality constraint Jacobian...:        0\n",
      "Number of nonzeros in inequality constraint Jacobian.:        0\n",
      "Number of nonzeros in Lagrangian Hessian.............:        0\n",
      "\n",
      "Total number of variables............................:        4\n",
      "                     variables with only lower bounds:        0\n",
      "                variables with lower and upper bounds:        0\n",
      "                     variables with only upper bounds:        0\n",
      "Total number of equality constraints.................:        0\n",
      "Total number of inequality constraints...............:        0\n",
      "        inequality constraints with only lower bounds:        0\n",
      "   inequality constraints with lower and upper bounds:        0\n",
      "        inequality constraints with only upper bounds:        0\n",
      "\n",
      "iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n",
      "   0  5.2023537e+02 0.00e+00 1.59e+01   0.0 0.00e+00    -  0.00e+00 0.00e+00   0\n",
      "   1  3.1411782e+02 0.00e+00 8.19e+00 -11.0 1.59e+01    -  1.00e+00 1.00e+00f  1\n",
      "   2  3.1082815e+02 0.00e+00 7.92e+00 -11.0 1.54e+01    -  1.00e+00 6.25e-02f  5\n",
      "   3  3.0779932e+02 0.00e+00 4.78e+00 -11.0 4.75e-01    -  1.00e+00 1.00e+00f  1\n",
      "   4  2.9745520e+02 0.00e+00 8.06e+00 -11.0 2.16e+00    -  1.00e+00 1.00e+00f  1\n",
      "   5  2.8953543e+02 0.00e+00 5.23e+00 -11.0 3.16e+00    -  1.00e+00 1.00e+00f  1\n",
      "   6  2.8818793e+02 0.00e+00 3.90e-01 -11.0 1.19e+00    -  1.00e+00 1.00e+00f  1\n",
      "   7  2.8792556e+02 0.00e+00 6.72e-01 -11.0 4.48e-01    -  1.00e+00 1.00e+00f  1\n",
      "   8  2.8728987e+02 0.00e+00 1.44e+00 -11.0 1.60e+00    -  1.00e+00 1.00e+00f  1\n",
      "   9  2.8606583e+02 0.00e+00 2.92e+00 -11.0 1.46e+01    -  1.00e+00 2.50e-01f  3\n",
      "iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n",
      "  10  2.8560848e+02 0.00e+00 3.87e+00 -11.0 4.89e+01    -  1.00e+00 3.12e-02f  6\n",
      "  11  2.8422538e+02 0.00e+00 1.29e+01 -11.0 1.48e+01    -  1.00e+00 2.50e-01f  3\n",
      "  12  2.8326002e+02 0.00e+00 2.40e+01 -11.0 1.93e+01    -  1.00e+00 6.25e-02f  5\n",
      "  13  2.8318418e+02 0.00e+00 3.23e+01 -11.0 1.77e+02    -  1.00e+00 7.81e-03f  8\n",
      "  14  2.8124512e+02 0.00e+00 7.88e+01 -11.0 7.29e+00    -  1.00e+00 1.25e-01f  4\n",
      "  15  2.7623569e+02 0.00e+00 1.43e+02 -11.0 4.86e+00    -  1.00e+00 1.25e-01f  4\n",
      "  16  2.7394546e+02 0.00e+00 2.22e+02 -11.0 4.63e+01    -  1.00e+00 7.81e-03f  8\n",
      "  17  2.7185947e+02 0.00e+00 3.48e+02 -11.0 2.56e+00    -  1.00e+00 1.25e-01f  4\n",
      "  18  2.6756317e+02 0.00e+00 3.28e+02 -11.0 5.00e-01    -  1.00e+00 2.50e-01f  3\n",
      "  19  2.4224351e+02 0.00e+00 2.63e+02 -11.0 5.29e-01    -  1.00e+00 1.00e+00f  1\n",
      "iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n",
      "  20  2.3380253e+02 0.00e+00 6.65e+02 -11.0 1.26e+00    -  1.00e+00 5.00e-01f  2\n",
      "  21  1.1424863e+02 0.00e+00 8.11e+02 -11.0 4.62e-01    -  1.00e+00 1.00e+00f  1\n",
      "  22  9.6120089e+01 0.00e+00 8.89e+02 -11.0 1.81e+00    -  1.00e+00 1.25e-01f  4\n",
      "  23  5.7435540e+01 0.00e+00 2.15e+02 -11.0 5.87e-01    -  1.00e+00 2.50e-01f  3\n",
      "  24  3.1403213e+01 0.00e+00 3.83e+02 -11.0 5.54e-01    -  1.00e+00 5.00e-01f  2\n",
      "  25  2.4891599e+01 0.00e+00 3.27e+02 -11.0 2.15e-01    -  1.00e+00 1.00e+00f  1\n",
      "  26  1.1034743e+01 0.00e+00 1.57e+02 -11.0 1.52e-01    -  1.00e+00 1.00e+00f  1\n",
      "  27  3.5861370e+00 0.00e+00 3.04e+01 -11.0 6.34e-02    -  1.00e+00 1.00e+00f  1\n",
      "  28  3.3335523e+00 0.00e+00 3.00e+01 -11.0 2.10e-02    -  1.00e+00 1.00e+00f  1\n",
      "  29  3.2536714e+00 0.00e+00 5.32e+00 -11.0 9.26e-03    -  1.00e+00 1.00e+00f  1\n",
      "iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n",
      "  30  3.2500293e+00 0.00e+00 7.86e+00 -11.0 2.48e-03    -  1.00e+00 5.00e-01f  2\n",
      "  31  3.2472825e+00 0.00e+00 1.88e+00 -11.0 5.04e-03    -  1.00e+00 2.50e-01f  3\n",
      "  32  3.2467768e+00 0.00e+00 1.02e-01 -11.0 6.09e-04    -  1.00e+00 1.00e+00f  1\n",
      "  33  3.2467752e+00 0.00e+00 5.26e-03 -11.0 3.35e-05    -  1.00e+00 1.00e+00f  1\n",
      "  34  3.2467752e+00 0.00e+00 1.23e-04 -11.0 7.97e-07    -  1.00e+00 1.00e+00f  1\n",
      "  35  3.2467752e+00 0.00e+00 3.51e-05 -11.0 2.24e-08    -  1.00e+00 1.00e+00f  1\n",
      "  36  3.2467752e+00 0.00e+00 1.80e-06 -11.0 8.91e-09    -  1.00e+00 1.00e+00f  1\n",
      "  37  3.2467752e+00 0.00e+00 1.59e-06 -11.0 9.26e-10    -  1.00e+00 1.00e+00f  1\n",
      "  38  3.2467752e+00 0.00e+00 7.38e-06 -11.0 1.91e-09    -  1.00e+00 1.00e+00f  1\n",
      "  39  3.2467752e+00 0.00e+00 4.21e-08 -11.0 1.57e-09    -  1.00e+00 1.00e+00f  1\n",
      "iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n",
      "  40  3.2467752e+00 0.00e+00 3.57e-09 -11.0 1.94e-11    -  1.00e+00 1.00e+00f  1\n",
      "\n",
      "Number of Iterations....: 40\n",
      "\n",
      "                                   (scaled)                 (unscaled)\n",
      "Objective...............:   3.2467751640171305e+00    3.2467751640171305e+00\n",
      "Dual infeasibility......:   3.5680711857333236e-09    3.5680711857333236e-09\n",
      "Constraint violation....:   0.0000000000000000e+00    0.0000000000000000e+00\n",
      "Complementarity.........:   0.0000000000000000e+00    0.0000000000000000e+00\n",
      "Overall NLP error.......:   3.5680711857333236e-09    3.5680711857333236e-09\n",
      "\n",
      "\n",
      "Number of objective function evaluations             = 161\n",
      "Number of objective gradient evaluations             = 41\n",
      "Number of equality constraint evaluations            = 0\n",
      "Number of inequality constraint evaluations          = 0\n",
      "Number of equality constraint Jacobian evaluations   = 0\n",
      "Number of inequality constraint Jacobian evaluations = 0\n",
      "Number of Lagrangian Hessian evaluations             = 0\n",
      "Total CPU secs in IPOPT (w/o function evaluations)   =      6.708\n",
      "Total CPU secs in NLP function evaluations           =      4.832\n",
      "\n",
      "EXIT: Optimal Solution Found.\n",
      "value.(mcue[:θ]) = [0.9892141158205213, 1.035423485821158, 0.9795184038350517, 0.9871519759905529]\n",
      "  173.815 ms (10809 allocations: 140.84 MiB)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Nothing[nothing, nothing, nothing, nothing], nothing)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function gmmObj(θ,gi,W)\n",
    "  g = gi(θ)\n",
    "  m = mean(g,dims=1)\n",
    "  (size(g)[1]*( m*W*m')[1]) # return scalar, not 1x1 array\n",
    "end\n",
    "\n",
    "function cue_jump(y,x,z)\n",
    "  n,d = size(x)\n",
    "  n,k = size(z)\n",
    "  cueobj = (θ...)->gmmObj([θ...],β->gi_ivshare(β,y,x,z),\n",
    "                          inv(cov(gi_ivshare([θ...],y,x,z))))\n",
    "  m = Model(with_optimizer(Ipopt.Optimizer, print_level=5))\n",
    "  @variable(m, θ[1:d])\n",
    "  JuMP.register(m, :cueobj, d, cueobj, autodiff=true)\n",
    "  @NLobjective(m,Min,cueobj(θ...)) \n",
    "  return(m)\n",
    "end\n",
    "@show mcue = cue_jump(y,x,z)\n",
    "set_start_value.(mcue[:θ], 2.0)\n",
    "optimize!(mcue)\n",
    "@show value.(mcue[:θ]) \n",
    "\n",
    "set_optimizer(mcue, with_optimizer(Ipopt.Optimizer,print_level=0))\n",
    "@btime (()->(set_start_value.(m[:θ], 2.0),\n",
    "             optimize!(mcue)))()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrap for EL\n",
    "\n",
    "For bootstrapping GMM, we discussed how it is important that the null\n",
    "hypothesis holds in the bootstrapped data. In GMM we did this by\n",
    "substracting the sample averages of the moments. In GEL, an\n",
    "alternative way to impose the null, is to sample the data with\n",
    "probabilities $\\hat{\\pi}_i$ instead of with equal proability. See\n",
    "Brown and Newey (2002)[@brown2002] for more information. \n",
    "\n",
    "# References\n",
    "\n",
    "\\bibliography"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.2.0",
   "language": "julia",
   "name": "julia-1.2"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
