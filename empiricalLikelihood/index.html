<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        <meta name="author" content="schrimpf">
        
        <link rel="shortcut icon" href="../img/favicon.ico">
        <title>EL - GMMInference.jl</title>
        <link href="../css/bootstrap-custom.min.css" rel="stylesheet">
        <link href="../css/font-awesome.min.css" rel="stylesheet">
        <link href="../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/atelier-forest-light.min.css">
        <link href="../assets/Documenter.css" rel="stylesheet">
        <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
        <!--[if lt IE 9]>
            <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
            <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
        <![endif]-->

        <script src="../js/jquery-1.10.2.min.js" defer></script>
        <script src="../js/bootstrap-3.0.3.min.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script> 
    </head>

    <body>

        <div class="navbar navbar-default navbar-fixed-top" role="navigation">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
                        <span class="sr-only">Toggle navigation</span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                    </button>
                    <a class="navbar-brand" href="..">GMMInference.jl</a>
                </div>

                <!-- Expanded navigation -->
                <div class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li >
                                <a href="..">Package Documentation</a>
                            </li>
                            <li >
                                <a href="../extremumEstimation/">Estimation</a>
                            </li>
                            <li >
                                <a href="../identificationRobustInference/">Inference</a>
                            </li>
                            <li class="active">
                                <a href="./">EL</a>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav navbar-right">
                            <li >
                                <a rel="next" href="../identificationRobustInference/">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="disabled">
                                <a rel="prev" >
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                            <li>
                                <a href="https://github.com/schrimpf/GMMInference.jl/edit/master/docs/empiricalLikelihood.md"><i class="fa fa-github"></i> Edit on GitHub</a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
                <div class="col-md-3"><div class="bs-sidebar hidden-print affix well" role="complementary">
    <ul class="nav bs-sidenav">
        <li class="main active"><a href="#about-this-document">About this document</a></li>
        <li class="main "><a href="#empirical-likelihood">Empirical likelihood</a></li>
            <li><a href="#gel-with-other-optimization-packages">GEL with other optimization packages</a></li>
        <li class="main "><a href="#inference-for-el">Inference for EL</a></li>
            <li><a href="#implementation">Implementation</a></li>
            <li><a href="#subvector-inference">Subvector inference</a></li>
            <li><a href="#bootstrap-for-el">Bootstrap for EL</a></li>
        <li class="main "><a href="#references">References</a></li>
    </ul>
</div></div>
                <div class="col-md-9" role="main">

<p><a href="http://creativecommons.org/licenses/by-sa/4.0/"><img alt="" src="https://i.creativecommons.org/l/by-sa/4.0/88x31.png" /></a></p>
<p>This work is licensed under a <a href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike
4.0 International
License</a> </p>
<h3 -="-" id="about-this-document">About this document<a class="headerlink" href="#about-this-document" title="Permanent link">&para;</a></h3>
<p>This document was created using Weave.jl. The code is available in
<a href="https://github.com/schrimpf/GMMInference.jl">on github</a>. The same
document generates both static webpages and associated <a href="../empiricalLikelihood.ipynb">jupyter
notebook</a>.</p>
<p>
<script type="math/tex; mode=display">
\def\indep{\perp\!\!\!\perp}
\def\Er{\mathrm{E}}
\def\R{\mathbb{R}}
\def\En{{\mathbb{E}_n}}
\def\Pr{\mathrm{P}}
\newcommand{\norm}[1]{\left\Vert {#1} \right\Vert}
\newcommand{\abs}[1]{\left\vert {#1} \right\vert}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\def\inprob{\,{\buildrel p \over \rightarrow}\,} 
\def\indist{\,{\buildrel d \over \rightarrow}\,} 
</script>
</p>
<h1 id="empirical-likelihood">Empirical likelihood<a class="headerlink" href="#empirical-likelihood" title="Permanent link">&para;</a></h1>
<p>An interesting alternative to GMM is (generalized) empirical
likelihood (GEL). Empirical likelihood has some appealing higher-order
statistical properties. In particular, it can be shown to have lower
higher order asymptotic bias than GMM. See Newey and Smith
(2004)<sup id="fnref:1"><a class="footnote-ref" href="#fn:1">1</a></sup>. Relatedly, certain test statistics based on EL are
robust to weak identification (Guggenberger and Smith
2005)<sup id="fnref:2"><a class="footnote-ref" href="#fn:2">2</a></sup>. In fact, the identification robust tests
that we have discusses are all based on the CUE-GMM objective
function. The CUE-GMM objetive is a special case of generalized
empirical likelihood.</p>
<p>A perceived downside of GEL is that it involves a more difficult
looking optimization problem than GMM. However, given the ease with
which Julia can solve high dimensional optimization problems, GEL is
very feasible. </p>
<p>As in the extremum estimation notes, suppose we have moment conditions
such that
<script type="math/tex; mode=display">
\Er[g_i(\theta)] = 0
</script>
where $g_i:\R^d \to \R^k$ are some data dependent moment
conditions. The empirical likelihood estimator solves
<script type="math/tex; mode=display">
\begin{align*}
    (\hat{\theta}, \hat{p}) = & \argmax_{\theta,p} \frac{1}{n} \sum_i
    \log(p_i) \;\; s.t.  \\
     & \sum_i p_i = 1, \;\; 0\leq p_i \leq 1 \\
     & \sum_i p_i g_i(\theta) = 0 
\end{align*}
</script>
</p>
<p>Generalized empirical likelihood replaces $\log(p)$ with some other
convex function $h(p)$, 
<script type="math/tex; mode=display">
\begin{align*}
    (\hat{\theta}^{GEL,h}, \hat{p}) = & \argmin_{\theta,p}
    \frac{1}{n}\sum_i h(p_i) \;\; s.t.  \\
     & \sum_i p_i = 1, \;\; 0\leq p \leq 1 \\
     & \sum_i p_i g_i(\theta) = 0 
\end{align*}
</script>
setting $h(p) = \frac{1}{2}(p^2-(1/n)^2)$ results in an estimator
identical to the CUE-GMM estimator.</p>
<p>A common approach to computing GEL estimators is to eliminate $\pi$ by
looking at the dual problem
<script type="math/tex; mode=display">
\hat{\theta}^{GEL}  = \argmin_{\theta}\sup_\lambda \sum_i \rho(\lambda'g_i(\theta))
</script>
where $\rho$ is some function related to $h$. See Newey and Smith (2004)<sup id="fnref2:1"><a class="footnote-ref" href="#fn:1">1</a></sup> for
details. There can be some analytic advantages to doing so, but
computationally, the original statement of the problem has some
advantages. First, there is more existing software for solving
constrained minimization problems than for solving saddle point
problems. Second, although $p$ is high dimensional, it enters the
constraints linearly, and the objective function is concave. Many
optimization algorithms will take good advantage of this. </p>
<p>Let&rsquo;s look at some Julia code. Since the problem involves many
variables with linear constraints, it is worthwhile to use JuMP for
optimization. The code is slightly more verbose, but the speed of
JuMP (and the Ipopt solver) are often worth it.</p>
<pre><code class="julia">using GMMInference, JuMP, Ipopt, LinearAlgebra, Distributions #, KNITRO 

n = 300
d = 4
k = 2*d
β0 = ones(d)
π0 = vcat(I,ones(k-d,d))
ρ = 0.5
data = IVLogitShare(n, β0, π0, ρ);
</code></pre>

<pre><code class="julia"># set up JuMP problem
Ty = quantile.(Logistic(),data.y)   
m = Model()
@variable(m, 0.0 &lt;= p[1:n] &lt;= 1.0)
@variable(m, θ[1:d])
@constraint(m, prob,sum(p)==1.0)
@constraint(m, momentcon[i=1:k], dot((Ty - data.x*θ).*data.z[:,i],p)==0.0)
@NLobjective(m,Max, sum(log(p[i]) for i in 1:n))
m
</code></pre>

<pre><code>A JuMP Model
Maximization problem with:
Variables: 304
Objective function type: Nonlinear
`GenericAffExpr{Float64,VariableRef}`-in-`MathOptInterface.EqualTo{Float64}
`: 1 constraint
`GenericQuadExpr{Float64,VariableRef}`-in-`MathOptInterface.EqualTo{Float64
}`: 8 constraints
`VariableRef`-in-`MathOptInterface.GreaterThan{Float64}`: 300 constraints
`VariableRef`-in-`MathOptInterface.LessThan{Float64}`: 300 constraints
Model mode: AUTOMATIC
CachingOptimizer state: NO_OPTIMIZER
Solver name: No optimizer attached.
Names registered in the model: momentcon, p, prob, θ
</code></pre>

<p>The <code>gel_jump_problem</code> function from <code>GMMInference.jl</code> does the same
thing as the above code cell. </p>
<p>Let&rsquo;s solve the optimization problem.</p>
<pre><code class="julia">set_optimizer(m, with_optimizer(Ipopt.Optimizer, print_level=5))
set_start_value.(m[:θ], 0.0)
set_start_value.(m[:p], 1/n)
optimize!(m)
</code></pre>

<pre><code>This is Ipopt version 3.12.10, running with linear solver mumps.
NOTE: Other linear solvers might be more efficient (see Ipopt documentation
).

Number of nonzeros in equality constraint Jacobian...:    21900
Number of nonzeros in inequality constraint Jacobian.:        0
Number of nonzeros in Lagrangian Hessian.............:     9900

Total number of variables............................:      304
                     variables with only lower bounds:        0
                variables with lower and upper bounds:      300
                     variables with only upper bounds:        0
Total number of equality constraints.................:        9
Total number of inequality constraints...............:        0
        inequality constraints with only lower bounds:        0
   inequality constraints with lower and upper bounds:        0
        inequality constraints with only upper bounds:        0

iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_p
r  ls
   0  1.3815514e+03 1.32e+01 2.42e-13  -1.0 0.00e+00    -  0.00e+00 0.00e+0
0   0
   1  1.7111437e+03 2.93e+00 4.80e+01  -1.0 3.61e-01    -  1.00e+00 1.00e+0
0h  1
   2  1.7111853e+03 4.60e-02 9.63e+00  -1.0 7.20e-01    -  1.00e+00 1.00e+0
0h  1
   3  1.7116324e+03 9.53e-05 7.55e+00  -1.0 8.22e-03    -  1.00e+00 1.00e+0
0h  1
   4  1.7116256e+03 1.36e-06 3.37e-01  -1.0 1.64e-03    -  1.00e+00 1.00e+0
0h  1
   5  1.7116256e+03 2.15e-10 3.97e-04  -1.7 1.49e-05    -  1.00e+00 1.00e+0
0h  1
   6  1.7116256e+03 1.04e-11 1.92e-05  -3.8 3.44e-06    -  1.00e+00 1.00e+0
0h  1
   7  1.7116256e+03 3.68e-14 4.56e-08  -5.7 2.05e-07    -  1.00e+00 1.00e+0
0h  1
   8  1.7116256e+03 7.86e-15 1.24e-13  -8.6 9.13e-11    -  1.00e+00 1.00e+0
0h  1

Number of Iterations....: 8

                                   (scaled)                 (unscaled)
Objective...............:   5.7054185876234601e+02    1.7116255762870380e+0
3
Dual infeasibility......:   1.2443708257960312e-13    3.7331124773880937e-1
3
Constraint violation....:   7.8639894075804850e-15    7.8639894075804850e-1
5
Complementarity.........:   2.5059044981856587e-09    7.5177134945569774e-0
9
Overall NLP error.......:   2.5059044981856587e-09    7.5177134945569774e-0
9


Number of objective function evaluations             = 9
Number of objective gradient evaluations             = 9
Number of equality constraint evaluations            = 9
Number of inequality constraint evaluations          = 0
Number of equality constraint Jacobian evaluations   = 9
Number of inequality constraint Jacobian evaluations = 0
Number of Lagrangian Hessian evaluations             = 8
Total CPU secs in IPOPT (w/o function evaluations)   =      0.022
Total CPU secs in NLP function evaluations           =      0.022

EXIT: Optimal Solution Found.
</code></pre>

<pre><code class="julia">@show value.(m[:θ])
</code></pre>

<pre><code>value.(m[:θ]) = [1.0746157775464957, 0.9445934187286001, 1.0124677882613653
, 0.9883924423214712]
</code></pre>

<pre><code class="julia">@show value.(m[:p][1:10])
</code></pre>

<pre><code>value.((m[:p])[1:10]) = [0.003271670198387547, 0.00348408609309951, 0.00338
17302438579567, 0.003343075736479741, 0.0036286307214915404, 0.003317614218
940237, 0.003615167221805361, 0.0034111127299104394, 0.0030392137179875663,
 0.003359507691098959]
10-element Array{Float64,1}:
 0.003271670198387547 
 0.00348408609309951  
 0.0033817302438579567
 0.003343075736479741 
 0.0036286307214915404
 0.003317614218940237 
 0.003615167221805361 
 0.0034111127299104394
 0.0030392137179875663
 0.003359507691098959
</code></pre>

<p>For comparison here is how long it takes JuMP + Ipopt to solve for the
CUE-GMM estimator. </p>
<pre><code class="julia">@show mcue = gmm_jump_problem(data, cue_objective)
</code></pre>

<pre><code>mcue = gmm_jump_problem(data, cue_objective) = A JuMP Model
Minimization problem with:
Variables: 4
Objective function type: Nonlinear
Model mode: AUTOMATIC
CachingOptimizer state: NO_OPTIMIZER
Solver name: No optimizer attached.
Names registered in the model: θ
</code></pre>

<pre><code class="julia">set_start_value.(mcue[:θ], 0.0)
set_optimizer(mcue,  with_optimizer(Ipopt.Optimizer,print_level=5))
optimize!(mcue)
</code></pre>

<pre><code>This is Ipopt version 3.12.10, running with linear solver mumps.
NOTE: Other linear solvers might be more efficient (see Ipopt documentation
).

Number of nonzeros in equality constraint Jacobian...:        0
Number of nonzeros in inequality constraint Jacobian.:        0
Number of nonzeros in Lagrangian Hessian.............:        0

Total number of variables............................:        4
                     variables with only lower bounds:        0
                variables with lower and upper bounds:        0
                     variables with only upper bounds:        0
Total number of equality constraints.................:        0
Total number of inequality constraints...............:        0
        inequality constraints with only lower bounds:        0
   inequality constraints with lower and upper bounds:        0
        inequality constraints with only upper bounds:        0

iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_p
r  ls
   0  1.5632786e+02 0.00e+00 5.19e+00   0.0 0.00e+00    -  0.00e+00 0.00e+0
0   0
   1  1.1973567e+02 0.00e+00 1.53e+01 -11.0 5.19e+00    -  1.00e+00 1.00e+0
0f  1
   2  1.0740274e+02 0.00e+00 5.76e+00 -11.0 4.01e+00    -  1.00e+00 1.00e+0
0f  1
   3  9.5859419e+01 0.00e+00 1.54e+00 -11.0 3.98e+00    -  1.00e+00 1.00e+0
0f  1
   4  9.4205844e+01 0.00e+00 1.47e+00 -11.0 1.21e+00    -  1.00e+00 1.00e+0
0f  1
   5  9.2358004e+01 0.00e+00 9.82e-01 -11.0 2.00e+00    -  1.00e+00 1.00e+0
0f  1
   6  9.0805145e+01 0.00e+00 1.42e+00 -11.0 4.23e+00    -  1.00e+00 1.00e+0
0f  1
   7  8.9798112e+01 0.00e+00 8.59e-01 -11.0 1.49e+00    -  1.00e+00 1.00e+0
0f  1
   8  8.9086117e+01 0.00e+00 5.51e-01 -11.0 1.96e+00    -  1.00e+00 1.00e+0
0f  1
   9  8.8963744e+01 0.00e+00 7.93e-01 -11.0 2.15e+01    -  1.00e+00 6.25e-0
2f  5
iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_p
r  ls
  10  8.7246868e+01 0.00e+00 5.47e-01 -11.0 5.16e+00    -  1.00e+00 1.00e+0
0f  1
  11  8.6668968e+01 0.00e+00 5.11e-01 -11.0 2.87e+00    -  1.00e+00 1.00e+0
0f  1
  12  8.6190719e+01 0.00e+00 3.96e-01 -11.0 5.55e+00    -  1.00e+00 5.00e-0
1f  2
  13  8.5512132e+01 0.00e+00 1.05e+01 -11.0 2.24e+01    -  1.00e+00 2.50e-0
1f  3
  14  8.5388344e+01 0.00e+00 7.45e+00 -11.0 3.91e+00    -  1.00e+00 1.00e+0
0f  1
  15  8.5380660e+01 0.00e+00 1.19e+00 -11.0 1.12e+01    -  1.00e+00 1.25e-0
1f  4
  16  8.5039599e+01 0.00e+00 2.76e+00 -11.0 1.44e+00    -  1.00e+00 5.00e-0
1f  2
  17  8.1397026e+01 0.00e+00 7.71e+00 -11.0 4.17e+00    -  1.00e+00 5.00e-0
1f  2
  18  7.2884178e+01 0.00e+00 6.34e+01 -11.0 1.08e+01    -  1.00e+00 1.25e-0
1f  4
  19  7.1260883e+01 0.00e+00 2.12e+02 -11.0 6.20e+01    -  1.00e+00 7.81e-0
3f  8
iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_p
r  ls
  20  5.9344765e+01 0.00e+00 2.02e+02 -11.0 2.12e+02    -  1.00e+00 4.88e-0
4f 12
  21  4.7113359e+01 0.00e+00 5.95e+01 -11.0 4.42e-02    -  1.00e+00 1.00e+0
0f  1
  22  4.4831085e+01 0.00e+00 6.33e+01 -11.0 1.68e-02    -  1.00e+00 1.00e+0
0f  1
  23  4.2438106e+01 0.00e+00 6.64e+01 -11.0 1.70e-02    -  1.00e+00 1.00e+0
0f  1
  24  3.9943690e+01 0.00e+00 6.96e+01 -11.0 1.73e-02    -  1.00e+00 1.00e+0
0f  1
  25  2.0851678e+01 0.00e+00 2.08e+02 -11.0 6.96e+01    -  1.00e+00 3.91e-0
3f  9
  26  1.7467765e+01 0.00e+00 8.56e+01 -11.0 1.81e+00    -  1.00e+00 1.25e-0
1f  4
  27  3.9826907e+00 0.00e+00 6.62e+01 -11.0 1.61e-01    -  1.00e+00 1.00e+0
0f  1
  28  2.1720000e+00 0.00e+00 4.33e+01 -11.0 1.19e-01    -  1.00e+00 1.00e+0
0f  1
  29  1.0062143e+00 0.00e+00 4.62e+00 -11.0 5.37e-02    -  1.00e+00 1.00e+0
0f  1
iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_p
r  ls
  30  9.7934461e-01 0.00e+00 3.30e+00 -11.0 8.45e-03    -  1.00e+00 1.00e+0
0f  1
  31  9.7586552e-01 0.00e+00 6.93e-01 -11.0 2.95e-03    -  1.00e+00 1.00e+0
0f  1
  32  9.7584110e-01 0.00e+00 5.50e-01 -11.0 3.27e-04    -  1.00e+00 1.00e+0
0f  1
  33  9.7576244e-01 0.00e+00 2.18e-02 -11.0 2.94e-04    -  1.00e+00 1.00e+0
0f  1
  34  9.7576214e-01 0.00e+00 3.32e-02 -11.0 6.06e-05    -  1.00e+00 1.00e+0
0f  1
  35  9.7576187e-01 0.00e+00 4.72e-03 -11.0 2.38e-05    -  1.00e+00 1.00e+0
0f  1
  36  9.7576187e-01 0.00e+00 8.92e-04 -11.0 4.07e-06    -  1.00e+00 1.00e+0
0f  1
  37  9.7576187e-01 0.00e+00 1.42e-04 -11.0 1.06e-06    -  1.00e+00 1.00e+0
0f  1
  38  9.7576187e-01 0.00e+00 5.35e-05 -11.0 2.12e-07    -  1.00e+00 1.00e+0
0f  1
  39  9.7576187e-01 0.00e+00 2.00e-06 -11.0 1.99e-08    -  1.00e+00 1.00e+0
0f  1
iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_p
r  ls
  40  9.7576187e-01 0.00e+00 1.21e-06 -11.0 4.75e-09    -  1.00e+00 1.00e+0
0f  1
  41  9.7576187e-01 0.00e+00 1.63e-07 -11.0 2.45e-09    -  1.00e+00 1.00e+0
0f  1
  42  9.7576187e-01 0.00e+00 1.00e-07 -11.0 1.72e-10    -  1.00e+00 1.00e+0
0f  1
  43  9.7576187e-01 0.00e+00 1.43e-07 -11.0 1.19e-10    -  1.00e+00 1.00e+0
0f  1
  44  9.7576187e-01 0.00e+00 1.28e-08 -11.0 3.75e-11    -  1.00e+00 1.00e+0
0f  1
  45  9.7576187e-01 0.00e+00 3.70e-09 -11.0 4.44e-12    -  1.00e+00 1.00e+0
0f  1

Number of Iterations....: 45

                                   (scaled)                 (unscaled)
Objective...............:   9.7576186878429783e-01    9.7576186878429783e-0
1
Dual infeasibility......:   3.7020220222672151e-09    3.7020220222672151e-0
9
Constraint violation....:   0.0000000000000000e+00    0.0000000000000000e+0
0
Complementarity.........:   0.0000000000000000e+00    0.0000000000000000e+0
0
Overall NLP error.......:   3.7020220222672151e-09    3.7020220222672151e-0
9


Number of objective function evaluations             = 134
Number of objective gradient evaluations             = 46
Number of equality constraint evaluations            = 0
Number of inequality constraint evaluations          = 0
Number of equality constraint Jacobian evaluations   = 0
Number of inequality constraint Jacobian evaluations = 0
Number of Lagrangian Hessian evaluations             = 0
Total CPU secs in IPOPT (w/o function evaluations)   =      2.810
Total CPU secs in NLP function evaluations           =      1.891

EXIT: Optimal Solution Found.
</code></pre>

<pre><code class="julia">@show value.(mcue[:θ])
</code></pre>

<pre><code>value.(mcue[:θ]) = [1.0750475340943448, 0.9442700157935878, 1.0115090241396
316, 0.9881720323877305]
4-element Array{Float64,1}:
 1.0750475340943448
 0.9442700157935878
 1.0115090241396316
 0.9881720323877305
</code></pre>

<p>In this comparison, EL is both faster and more robust to initial
values than CUE-GMM. GMM with a fixed weighting matrix will likely be
faster than either.</p>
<h2 id="gel-with-other-optimization-packages">GEL with other optimization packages<a class="headerlink" href="#gel-with-other-optimization-packages" title="Permanent link">&para;</a></h2>
<p>We can also estimate GEL models with other optimization
packages. Relative to JuMP, other optimization packages have the
advantage that the problem does not have to be written in any special
syntax. However, other packages have the downside that they will not
recognize any special structure in the constraints (linear, quadratic,
sparse, etc) unless we explicitly provide it. Let&rsquo;s see how much, if
any difference this makes to performance. </p>
<p>Here we will use the <code>NLPModels.jl</code> interface to
Ipopt. Essentially, all this does is call <code>ForwardDiff</code> on the
objective function and constraints, and then give the resulting
gradient and hessian functions to Ipopt.</p>
<pre><code class="julia">using NLPModelsIpopt, NLPModels #NLPModelsKnitro, 
gel = gel_nlp_problem(data)
ip = ipopt(gel)
</code></pre>

<pre><code>This is Ipopt version 3.12.10, running with linear solver mumps.
NOTE: Other linear solvers might be more efficient (see Ipopt documentation
).

Number of nonzeros in equality constraint Jacobian...:     2736
Number of nonzeros in inequality constraint Jacobian.:        0
Number of nonzeros in Lagrangian Hessian.............:    46360

Total number of variables............................:      304
                     variables with only lower bounds:        0
                variables with lower and upper bounds:      304
                     variables with only upper bounds:        0
Total number of equality constraints.................:        9
Total number of inequality constraints...............:        0
        inequality constraints with only lower bounds:        0
   inequality constraints with lower and upper bounds:        0
        inequality constraints with only upper bounds:        0

iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_p
r  ls
   0  1.3815514e+03 2.00e+00 1.85e-13  -1.0 0.00e+00    -  0.00e+00 0.00e+0
0   0
   1  1.7115975e+03 5.18e-02 8.15e+01  -1.0 2.46e-02    -  9.90e-01 1.00e+0
0h  1
   2  1.7116316e+03 1.63e-04 3.27e+00  -1.0 4.88e-02    -  9.90e-01 1.00e+0
0h  1
   3  1.7116257e+03 6.98e-06 3.70e-02  -1.0 1.25e-03    -  9.90e-01 1.00e+0
0h  1
   4  1.7116256e+03 1.67e-10 1.86e-04  -1.0 6.24e-06    -  9.90e-01 1.00e+0
0h  1
   5  1.7116256e+03 1.61e-16 1.67e-06  -1.0 5.96e-10    -  9.91e-01 1.00e+0
0h  1
   6  1.7116256e+03 3.06e-16 4.28e-08  -1.7 3.62e-09    -  1.00e+00 1.00e+0
0h  1
   7  1.7116256e+03 1.16e-16 6.23e-09  -3.8 2.13e-09    -  1.00e+00 1.00e+0
0h  1
   8  1.7116256e+03 1.43e-16 3.04e-12  -5.7 1.38e-10    -  1.00e+00 1.00e+0
0h  1
   9  1.7116256e+03 2.35e-16 5.94e-14  -8.6 2.02e-13    -  1.00e+00 1.00e+0
0h  1

Number of Iterations....: 9

                                   (scaled)                 (unscaled)
Objective...............:   5.7054185876234624e+02    1.7116255762870387e+0
3
Dual infeasibility......:   5.9363616529322706e-14    1.7809084958796812e-1
3
Constraint violation....:   2.3462135012586316e-16    2.3462135012586316e-1
6
Complementarity.........:   2.5059035609395841e-09    7.5177106828187526e-0
9
Overall NLP error.......:   2.5059035609395841e-09    7.5177106828187526e-0
9


Number of objective function evaluations             = 10
Number of objective gradient evaluations             = 10
Number of equality constraint evaluations            = 10
Number of inequality constraint evaluations          = 0
Number of equality constraint Jacobian evaluations   = 10
Number of inequality constraint Jacobian evaluations = 0
Number of Lagrangian Hessian evaluations             = 9
Total CPU secs in IPOPT (w/o function evaluations)   =      0.208
Total CPU secs in NLP function evaluations           =    309.039

EXIT: Optimal Solution Found.
&quot;Execution stats: first-order stationary&quot;
</code></pre>

<p>As you see, this approach is far slower than with JuMP. Notice that
the number of iterations and function evaluations are identical. The
big difference is that JuMP evaluates the function (and its
derivatives) very quickly, while NLP takes much much longer. I would
guess that this is largely because it is using ForwardDiff to
calculate a gradients and hessians for 304 variables.</p>
<p>Let&rsquo;s also estimate the model using <code>Optim.jl</code>. </p>
<pre><code class="julia">using Optim
args = gel_optim_args(data)
@time opt=optimize(args[1],args[2],
                   [fill(1/(1.1*n),n)..., zeros(d)...],
                   IPNewton(μ0=:auto, show_linesearch=false),
                   Optim.Options(show_trace=true,
                                 allow_f_increases=true,
                                 successive_f_tol=10,
                                 allow_outer_f_increases=true))
</code></pre>

<pre><code>Iter     Lagrangian value Function value   Gradient norm    |==constr.|    
  μ
     0   7.760253e+03     1.739728e+03     6.267395e+04     6.006251e+03   
  8.19e-03
 * time: 1.572234055755275e9
     1   5.742873e+03     1.734280e+03     2.785824e+05     4.007169e+03   
  8.19e-04
 * time: 1.572234056273661e9
     2   3.579963e+03     1.764024e+03     2.124065e+05     1.814407e+03   
  8.63e-04
 * time: 1.572234056325907e9
     3   3.552294e+03     1.765061e+03     2.109796e+05     1.780627e+03   
  3.72e-03
 * time: 1.572234056333738e9
     4   2.412066e+03     1.903421e+03     5.380874e+05     5.016661e+02   
  3.66e-03
 * time: 1.572234056357021e9
     5   2.277038e+03     2.002041e+03     3.777955e+05     2.712601e+02   
  1.86e-03
 * time: 1.572234056365182e9
     6   2.257185e+03     2.010401e+03     3.195740e+05     2.447038e+02   
  1.03e-03
 * time: 1.572234056386374e9
     7   2.249367e+03     2.022001e+03     3.051330e+05     2.237677e+02   
  1.76e-03
 * time: 1.572234056394575e9
     8   2.220244e+03     2.044474e+03     2.129540e+05     1.722936e+02   
  1.69e-03
 * time: 1.572234056402378e9
     9   2.192982e+03     2.049654e+03     1.773300e+05     1.404642e+02   
  1.39e-03
 * time: 1.572234056422108e9
    10   2.136006e+03     2.058862e+03     1.256052e+05     7.546297e+01   
  8.11e-04
 * time: 1.572234056434702e9
    11   2.066586e+03     2.059401e+03     7.116003e+04     5.757864e+00   
  6.88e-04
 * time: 1.572234056457992e9
    12   2.049396e+03     2.045453e+03     6.174964e+04     3.556395e+00   
  1.88e-04
 * time: 1.572234056465959e9
    13   1.999420e+03     2.007586e+03     4.417601e+04     8.650170e+00   
  2.40e-04
 * time: 1.572234056473884e9
    14   1.871874e+03     1.905001e+03     3.357934e+04     3.359914e+01   
  2.46e-04
 * time: 1.572234056493636e9
    15   1.832819e+03     1.862911e+03     2.857589e+04     3.043513e+01   
  1.83e-04
 * time: 1.572234056501689e9
    16   1.814294e+03     1.841183e+03     2.642790e+04     2.704772e+01   
  8.55e-05
 * time: 1.572234056521602e9
    17   1.795134e+03     1.817989e+03     2.368806e+04     2.300841e+01   
  8.37e-05
 * time: 1.572234056529804e9
    18   1.772282e+03     1.790961e+03     2.048296e+04     1.883554e+01   
  8.63e-05
 * time: 1.57223405654927e9
    19   1.739555e+03     1.752928e+03     1.557820e+04     1.355519e+01   
  1.03e-04
 * time: 1.572234056557142e9
    20   1.730773e+03     1.736314e+03     1.158612e+04     5.584439e+00   
  2.51e-05
 * time: 1.572234056565132e9
    21   1.721892e+03     1.722684e+03     7.251435e+03     8.248475e-01   
  1.89e-05
 * time: 1.572234056585037e9
    22   1.715061e+03     1.714993e+03     3.451323e+03     5.166879e-02   
  9.56e-06
 * time: 1.57223405659301e9
    23   1.713570e+03     1.713551e+03     2.307286e+03     1.569695e-02   
  2.02e-06
 * time: 1.572234056612596e9
    24   1.712554e+03     1.712549e+03     1.272115e+03     2.295297e-03   
  1.33e-06
 * time: 1.572234056620333e9
    25   1.712039e+03     1.712037e+03     7.843087e+02     3.891663e-04   
  4.19e-07
 * time: 1.572234056628041e9
    26   1.711731e+03     1.711731e+03     3.791625e+02     4.095774e-05   
  1.96e-07
 * time: 1.572234056647892e9
    27   1.711640e+03     1.711640e+03     3.056792e+02     9.722189e-05   
  4.41e-08
 * time: 1.572234056656095e9
    28   1.711626e+03     1.711626e+03     1.619330e+02     4.553762e-05   
  2.15e-08
 * time: 1.572234056675595e9
    29   1.711626e+03     1.711626e+03     2.640030e+02     2.328054e-05   
  4.41e-09
 * time: 1.572234056683574e9
    30   1.711626e+03     1.711626e+03     2.492295e+02     1.177771e-05   
  3.28e-09
 * time: 1.572234056702814e9
    31   1.711626e+03     1.711626e+03     2.500228e+02     5.924700e-06   
  1.86e-09
 * time: 1.57223405671072e9
    32   1.711626e+03     1.711626e+03     2.492047e+02     2.971566e-06   
  1.09e-09
 * time: 1.572234056718681e9
    33   1.711626e+03     1.711626e+03     2.489765e+02     1.488122e-06   
  6.40e-10
 * time: 1.572234056738594e9
    34   1.711626e+03     1.711626e+03     2.488402e+02     7.446523e-07   
  3.75e-10
 * time: 1.572234056746683e9
    35   1.711626e+03     1.711626e+03     2.487789e+02     3.724753e-07   
  2.19e-10
 * time: 1.572234056765375e9
    36   1.711626e+03     1.711626e+03     2.487504e+02     1.862753e-07   
  1.28e-10
 * time: 1.572234056773395e9
    37   1.711626e+03     1.711626e+03     2.487386e+02     9.314709e-08   
  7.52e-11
 * time: 1.572234056793108e9
    38   1.711626e+03     1.711626e+03     1.243768e+02     7.324104e-12   
  4.40e-11
 * time: 1.572234056801188e9
    39   1.711626e+03     1.711626e+03     2.487539e+02     3.334440e-14   
  7.52e-12
 * time: 1.572234056809012e9
    40   1.711626e+03     1.711626e+03     1.243769e+02     2.734703e-14   
  4.40e-12
 * time: 1.572234056828486e9
    41   1.711626e+03     1.711626e+03     2.621549e+02     9.998092e-14   
  7.52e-13
 * time: 1.572234056844296e9
    42   1.711626e+03     1.711626e+03     6.218847e+01     2.042014e-14   
  5.96e-13
 * time: 1.57223405689223e9
    43   1.711626e+03     1.711626e+03     2.665310e+02     2.477567e-11   
  7.52e-14
 * time: 1.57223405690093e9
    44   1.711626e+03     1.711626e+03     2.662522e+02     3.492199e-11   
  6.74e-14
 * time: 1.572234056909789e9
    45   1.711626e+03     1.711626e+03     2.699999e+02     3.492371e-11   
  5.99e-14
 * time: 1.572234056932068e9
    46   1.711626e+03     1.711626e+03     2.699998e+02     3.492602e-11   
  5.99e-14
 * time: 1.572234056951078e9
  5.191920 seconds (10.19 M allocations: 916.861 MiB, 15.86% gc time)
</code></pre>

<pre><code class="julia">@show opt.minimizer[(n+1):end]
</code></pre>

<pre><code>opt.minimizer[n + 1:end] = [1.074615777676794, 0.9445934188229393, 1.012467
7883625433, 0.9883924423856543]
4-element Array{Float64,1}:
 1.074615777676794 
 0.9445934188229393
 1.0124677883625433
 0.9883924423856543
</code></pre>

<p>The <code>IPNewton()</code> optimizer from <code>Optim.jl</code> appears to be much less
efficient than <code>Ipopt</code>. <code>IPNewton</code> takes more than 4 times as many
iterations. </p>
<h1 id="inference-for-el">Inference for EL<a class="headerlink" href="#inference-for-el" title="Permanent link">&para;</a></h1>
<p>Guggenberger and Smith (2005)<sup id="fnref2:2"><a class="footnote-ref" href="#fn:2">2</a></sup> show that GEL
versions of the AR and LM statistics are robust to weak
identification. The GEL version of the AR statistic is the generalized
empirical likelihood ratio. Guggenberger and Smith (2005) show that</p>
<p>
<script type="math/tex; mode=display">
GELR(\theta_0) = 2\sum_{i=1}^n\left(h(p_i(\theta_0))  -
   h(1/n)\right) \indist \chi^2_k 
</script>
</p>
<p>where $p_i(\theta_0)$ are given by</p>
<p>
<script type="math/tex; mode=display">
\begin{align*}
    p(\theta)  =  \argmax_{0 \leq p \leq 1} \sum h(p_i) \text{ s.t. } & \sum_i p_i
    = 1 \\
& \sum_i p_i g_i(\theta) = 0
\end{align*}
</script>
</p>
<p>The GELR statistic shares the downsides of the AR statistic &mdash; the
degrees of freedom is the number of moments instead of the number of
parameters, which tends to lead to lower power in overidentified
models; and it combines a test of misspecification with a location
test for $\theta$. </p>
<p>Consequently, it can be useful to instead look at a Lagrange
multiplier style statistic. The true $\theta$ maximizes the
empirical likelihood, so </p>
<p>
<script type="math/tex; mode=display">
0 = \sum_{i=1}^n \nabla_\theta h(p_i(\theta_0)) = \lambda(\theta_0)' \sum_{i=1}^n
p_i(\theta_0) \nabla_\theta g_i(\theta_0) \equiv \lambda(\theta_0) D(\theta_0)
</script>
</p>
<p>where $p_i(\theta_0)$ is as defined above, and $\lambda(\theta_0)$ are
the mulitpliers on the empirical moment condition constraint. Andrews
and Guggenberger show that a quadratic form in the above score
equation is asymptotically $\chi^2_d$. To be specific, let
$\Delta(\theta) = E[(1/n\sum_i g_i(\theta) - E[g(\theta)])(1/n \sum_i
 g_i(\theta) - E[g(\theta)])&rsquo;]$  and define</p>
<p>
<script type="math/tex; mode=display">
S(\theta) = \lambda(\theta)' D(\theta) \left( D(\theta)
\Delta(\theta)^{-1} D(\theta) \right)^{-1} D(\theta)'\lambda(\theta)
</script>
</p>
<p>then $S(\theta_0) \indist \chi^2_d$. This result holds whether or not
$\theta$ is strongly identified. </p>
<h3 id="implementation">Implementation<a class="headerlink" href="#implementation" title="Permanent link">&para;</a></h3>
<p>Computing the $GELR$ and $S$ statistics requires solving a linear
program for each $\theta$ we want to test. Fortunately, linear
programs can be solved very quickly. See <code>gel_pλ</code> in <code>GMMInference.jl</code>
for the relevant code. </p>
<p>Let&rsquo;s do a simulation to check that these tests have correct coverage.</p>
<pre><code class="julia">using Plots
S = 500
n = 200
d = 2
β0 = ones(d)
ρ = 0.5
k = 3
function sim_p(π0)
  data = IVLogitShare(n, β0, π0, ρ)
  GELR, S, plr, ps = gel_tests(β0, data)
  [plr ps]
end
πweak = ones(k,d) .+ vcat(diagm(0=&gt;fill(0.01,d)),zeros(k-d,d))  
πstrong = vcat(5*diagm(0=&gt;ones(d)),ones(k-d,d)) 
pweak=vcat([sim_p(πweak ) for s in 1:S]...)
pstrong=vcat([sim_p(πstrong) for s in 1:S]...)

pgrid = 0:0.01:1
plot(pgrid, p-&gt;(mean( pstrong[:,1] .&lt;= p)-p), legend=:topleft,
     label=&quot;GELR, strong ID&quot;, style=:dash, color=:blue,
     xlabel=&quot;p&quot;, ylabel=&quot;P(p value &lt; p)-p&quot;,
     title=&quot;Simulated CDF of p-values - p&quot;)  
plot!(pgrid, p-&gt;(mean( pstrong[:,2] .&lt;= p)-p),
      label=&quot;S, strong ID&quot;, style=:dash, color=:green)

plot!(pgrid, p-&gt;(mean( pweak[:,1] .&lt;= p)-p),
      label=&quot;GELR, weak ID&quot;, style=:solid, color=:blue)
plot!(pgrid, p-&gt;(mean( pweak[:,2] .&lt;= p)-p),
      label=&quot;S, weak ID&quot;, style=:solid, color=:green)
plot!(pgrid,0,alpha=0.5, label=&quot;&quot;)
</code></pre>

<pre><code>Simulated CDF of p-v
alues - p
                                         ┌─────────────────────────────────
────────────────────────┐                
                    0.016340000000000014 │⠀⢸⠀⠀⢠⠀⠀⠀⠀⡀⠀⡀⠀⠀⣤⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀
⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ GELR, strong ID
                                         │⠀⢸⠀⠀⢸⢣⠀⠀⡜⢇⢀⢧⡀⢀⠟⡆⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀
⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ S, strong ID   
                                         │⠀⢸⢀⠀⣇⠀⣷⣼⠲⡜⢺⣾⣟⣼⠀⣣⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀
⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ GELR, weak ID  
                                         │⣀⣸⣫⣶⣟⣆⣸⣁⣀⣀⣰⣷⣹⣎⣟⣟⣄⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀
⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣰⣄⣀│ S, weak ID     
                                         │⠀⢸⡸⠙⣄⠏⡞⢀⡄⢰⠉⠀⠈⠞⠸⢧⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀
⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡜⡄⠀⢸⡇⠀│                
                                         │⠀⢸⠟⢄⠟⠀⠱⢊⢧⣸⠀⠀⡤⢄⠀⠀⢷⢄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀
⠀⠀⢀⡄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢠⠃⢳⣽⣼⠇⠀│                
                                         │⠀⢸⠀⠸⣀⠀⡸⠉⠸⡿⡀⡷⠁⠈⡆⡀⠸⣸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀
⠀⠀⢸⠸⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡸⠀⡜⢱⣿⠀⠀│                
                                         │⠀⢸⠀⠀⠋⠓⠁⠀⠀⠀⣧⠃⠀⠀⣿⢱⠀⢧⡇⢀⡀⠀⢀⡆⡀⠀⠀⠀⠀⠀⠀⠀⠀
⠀⠀⡎⠀⢣⠀⡆⡀⢀⠦⡀⠀⠀⠀⠀⣆⢠⡷⣀⡇⠈⡟⠀⠀│                
                                         │⠀⢸⠀⠀⠀⠀⠀⠀⠀⠀⢸⠀⠀⠀⠀⠘⡄⢸⢳⣊⣿⠔⡞⢱⢇⠀⠀⠀⠀⠀⠀⠀⠀
⠀⠀⡇⠀⠘⡼⣱⠛⣼⠀⣇⠀⠀⠀⢰⢹⣸⡇⠀⣀⢰⠃⠀⠀│                
   P(p value &lt; p)-p                      │⠀⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢱⠀⠃⢏⠓⢴⣧⠀⢸⣶⠀⠀⠀⠀⠀⠀⠀
⠀⠀⡇⠀⠀⡇⢻⡲⣿⣼⢾⠀⠀⠀⡎⠀⡿⢇⠞⢸⠏⠀⠀⠀│                
                                         │⠀⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢱⠀⠸⡀⡎⠸⣠⡸⢫⡆⠀⠀⠀⠀⠀⠀
⡄⡜⠀⠀⣸⠉⠁⠁⡿⣿⠘⡄⠀⢠⣇⢸⡤⣮⠀⡎⠀⠀⠀⠀│                
                                         │⠀⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠘⣤⠀⠉⠀⠀⡟⠧⢻⡧⣜⡆⠀⠀⠀⣸
⢾⡏⢱⣤⡟⠀⠀⠀⠁⠁⠀⣷⣠⣼⠈⡾⠗⠁⠳⠁⠀⠀⠀⠀│                
                                         │⠀⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠛⡄⠀⠀⡏⢹⠀⠘⣷⢹⣈⢹⡓⢲⡇
⠈⡖⠊⠛⠃⠀⠀⣀⢦⢦⡰⣾⡏⡟⣜⠇⠀⠀⠀⠀⠀⠀⠀⠀│                
                                         │⠀⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡷⣄⠜⠀⠀⠓⡆⢸⢦⣿⣇⢣⠃⠀
⠀⡇⠀⠀⠀⠀⢠⠃⠀⠸⡇⠘⣟⢿⠃⠀⠀⠀⠀⠀⠀⠀⠀⠀│                
                                         │⠀⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⢦⠀⣿⠛⡄⠀⡰
⣤⡃⠀⠀⠀⠀⡜⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│                
                                         │⠀⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠉⠀⢣⠳⠳⠁
⡏⠸⣀⡄⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│                
                                         │⠀⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⡆⠀⡼
⠀⠀⣿⠳⢇⢀⠇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│                
                                         │⠀⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠱⠳⠁
⠀⠀⠁⠀⢸⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│                
                    -0.06634000000000007 │⠀⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀
⠀⠀⠀⠀⠀⠋⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│                
                                         └─────────────────────────────────
────────────────────────┘                
                                         -0.03                             
                     1.03
                                                                     p
</code></pre>

<h3 id="subvector-inference">Subvector inference<a class="headerlink" href="#subvector-inference" title="Permanent link">&para;</a></h3>
<p>Guggenberger and Smith (2005)<sup id="fnref3:2"><a class="footnote-ref" href="#fn:2">2</a></sup> also give results for
subvector inference. Let $(\alpha, \beta) =\theta$. Assume $\beta$ is
strongly identified. Guggenberger and Smith show that analogs of
$GELR$ and $S$ with $\beta$ concentrated out lead to valid tests for
$\alpha$, whether $\alpha$ is weakly or strongly identified. </p>
<h2 id="bootstrap-for-el">Bootstrap for EL<a class="headerlink" href="#bootstrap-for-el" title="Permanent link">&para;</a></h2>
<p>For bootstrapping GMM, we discussed how it is important that the null
hypothesis holds in the bootstrapped data. In GMM we did this by
substracting the sample averages of the moments. In GEL, an
alternative way to impose the null, is to sample the data with
probabilities $\hat{p}_i$ instead of with equal proability. See
Brown and Newey (2002)<sup id="fnref:3"><a class="footnote-ref" href="#fn:3">3</a></sup> for more information. </p>
<h1 id="references">References<a class="headerlink" href="#references" title="Permanent link">&para;</a></h1>
<div class="footnote">
<hr />
<ol>
<li id="fn:1">
<p>Whitney K Newey and Richard J Smith. Higher order properties of gmm and generalized empirical likelihood estimators. <em>Econometrica</em>, 72<script type="math/tex">1</script>:219–255, 2004. URL: <a href="https://www.jstor.org/stable/pdf/3598854.pdf?casa_token=m7RohRNDzqcAAAAA:rOXGanSJq8u2gpjAg1T-K_dJxhE_PoNwtFNQGV6YHwZ_0WlTnc_9jj5qbCeGpo6ekF1WGEcBPG9wLelRbrBziziFD6inZ_W_wRuza_UZKJzJSYnwtw7vOA">https://www.jstor.org/stable/pdf/3598854.pdf?casa_token=m7RohRNDzqcAAAAA:rOXGanSJq8u2gpjAg1T-K_dJxhE_PoNwtFNQGV6YHwZ_0WlTnc_9jj5qbCeGpo6ekF1WGEcBPG9wLelRbrBziziFD6inZ_W_wRuza_UZKJzJSYnwtw7vOA</a>.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:1" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:2">
<p>Patrik Guggenberger and Richard J. Smith. Generalized empirical likelihood estimators and tests under partial, weak, and strong identification. <em>Econometric Theory</em>, 21<script type="math/tex">4</script>:667–709, 2005. <a href="https://doi.org/10.1017/S0266466605050371">doi:10.1017/S0266466605050371</a>.&#160;<a class="footnote-backref" href="#fnref:2" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:2" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:2" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
<li id="fn:3">
<p>Bryan W Brown and Whitney K Newey. Generalized method of moments, efficient bootstrapping, and improved inference. <em>Journal of Business &amp; Economic Statistics</em>, 20<script type="math/tex">4</script>:507&ndash;517, 2002. URL: <a href="https://doi.org/10.1198/073500102288618649">https://doi.org/10.1198/073500102288618649</a>, <a href="https://arxiv.org/abs/https://doi.org/10.1198/073500102288618649">arXiv:https://doi.org/10.1198/073500102288618649</a>, <a href="https://doi.org/10.1198/073500102288618649">doi:10.1198/073500102288618649</a>.&#160;<a class="footnote-backref" href="#fnref:3" title="Jump back to footnote 3 in the text">&#8617;</a></p>
</li>
</ol>
</div></div>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../assets/mathjaxhelper.js" defer></script>

        <div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="Keyboard Shortcuts Modal" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
                <h4 class="modal-title" id="exampleModalLabel">Keyboard Shortcuts</h4>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
